# Нейронный сети

## BackPropagation

### Основные рекомендации обучения сети

- Запускать алгоритм для разных начальных значений весовых коэффициентов. Затем отобрать лучший вариант. Начальное значение генерируем случайным образом, в окрестности нуля (чтобы не попасть в область где значение производной близко к нулю, что приведет к малому изменению весовых коэффициентов в процессе обучения), кроме тех что относятся к bias (bias - смещение проскости/гиперплоскости);
- Запускать алгоритм обучения с оптимизацией по Adam или Нестерову для ускорения обучения;
- Выполнять нормировку входных значений и запоминать нормировочные параметры: min, max из обучающей выборки, для применения на практических выходных значениях;
- Помещать в обучающую выборку разнообразные данные примерно равного количества;
- Наблюдения на вход сети подавать случайным образом, корректировать веса осле серии наблюдений, разбитых на mini-batch (mini-batch - выборка около 1000 экземпляров);
- Использовать минимальное необходимое количество нейронов в сети, для избежания эффекта переобучения (когда сеть слишком сильно подстраивается под обучающую выборку);
- Разбивать все множество наблюдений на три выборки: обучающую, валидирующую, тестовую. Это необходимо для предотвращения эффекта переобучения.

### Критерии остановки процесса обучения сети

- При расхождении параметров качества обучающей и валидирующей выборки;
- От итерации к итерации (по всей эпохе) показатель качества выходит на плато. В таком случае можно: переопределить параметры для градиентного алгоритма, переиннициализировать сеть с другими весовыми коэффициентами;
- Малое изменение весовых коэффициентов;
- Достигли максимального числа итераций.
